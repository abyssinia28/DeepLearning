{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole-Problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abyssinia28/deeplearning/blob/master/CartPole_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1t3nkbUGle7",
        "colab_type": "text"
      },
      "source": [
        "# The CartPole Problem\n",
        "\n",
        "The agent in the cartpole problem is the cart. The observations include the cart position, cart velocity, pole angle, and pole velocity at the tip of the pole. The two possible actions that can be taken by the agent are: pushing the cart to the left or to the right. For every second the pole stays upright, the agent receives +1 as a reward, otherwise it is penalized by -1. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdWxbHqkP5Ra",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# 1.   Importing libraries\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJC5IJ48lnFt",
        "colab_type": "code",
        "outputId": "5791aecf-c2b8-4dc4-ff33-6b169ebadd36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jo6ZPnkQUCG",
        "colab_type": "text"
      },
      "source": [
        "# 2. Setting up the OpenAI gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBwBEcZrmxvl",
        "colab_type": "code",
        "outputId": "4158cd68-8766-43da-c8b4-3460bd7a82c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "for i_episode in range(20): #the number of episodes that we will run\n",
        "  observation = env.reset() #reset to start from the beginning\n",
        "  for t in range(1000):\n",
        "    #env.render()\n",
        "    #print(observation)\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    # if done:\n",
        "    #   print(\"Episode finished after {} timesteps\". format(t+1))\n",
        "    #   break\n",
        "    \n",
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.high)\n",
        "print(env.observation_space.low)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Discrete(2)\n",
            "Box(4,)\n",
            "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
            "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saAq3ESpQum4",
        "colab_type": "text"
      },
      "source": [
        "# 3. Defining Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cbos4bxnbbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training parameters\n",
        "\n",
        "n_episodes = 500 \n",
        "n_win_ticks = 195 \n",
        "max_env_steps = None \n",
        "\n",
        "\n",
        "gamma = 1.0 #discount factor - consideration of future rewards - set 1 for same policies \n",
        "epsilon = 1.0 #exploration - choosing an action with best long term effect, \n",
        "#when 1.0 it chooses a uniformly random choice  \n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995 #controls how quickly it stops exploring\n",
        "alpha = 0.01 #learning rate - the new info overrides the old\n",
        "#1.0 we only consider the recent result\n",
        "alpha_decay = 0.01\n",
        "\n",
        "batch_size = 64\n",
        "monitor = False\n",
        "quiet = False #controls our printing statements\n",
        "\n",
        "#environment parameters\n",
        "\n",
        "memory = deque(maxlen = 100000)\n",
        "env = gym.make('CartPole-v0')\n",
        "if max_env_steps is not None: env.max_episode_steps = max_env_steps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2sJ_xl4Stzh",
        "colab_type": "text"
      },
      "source": [
        "# 4. Building the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ2DuNfxnd7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model definition\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(48, input_dim = 4, activation = 'tanh')) #environment has 4 parameters\n",
        "model.add(Dense(48, activation = 'tanh'))\n",
        "model.add(Dense(2, activation = 'linear'))\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr = alpha, decay = alpha_decay))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbsl4HjVWoxk",
        "colab_type": "text"
      },
      "source": [
        "# 5. Defining necessary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG0NLFIoni0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define necessary functions\n",
        "\n",
        "def remember(state, action, reward, next_state, done):\n",
        "  \"\"\" \n",
        "  Setting up the memory\n",
        "  \"\"\"\n",
        "  memory.append((state, action, reward, next_state, done)) \n",
        "    \n",
        "def choose_action(state, epsilon): \n",
        "    \"\"\"\n",
        "    pick what to do based on state and exploration factor\n",
        "    returns action space sample - if the randomly generater number is less\n",
        "    than epsilon, it will give the action space sample, if not, the model will\n",
        "    make a prediction based off the current state\n",
        "    \"\"\"\n",
        "    return env.action_space.sample() if (np.random.random() <= epsilon) \\\n",
        "        else np.argmax(model.predict(state))\n",
        "def get_epsilon(t):\n",
        "    \"\"\"\n",
        "    towards the end we will be decreasing substantially\n",
        "    \"\"\"\n",
        "    return max(epsilon_min, min(epsilon, 1.0 - math.log10((t+1)*epsilon_decay)))\n",
        "\n",
        "def preprocess_state(state):\n",
        "    \"\"\"\n",
        "    make sure that it is in the right input format \n",
        "    \"\"\"\n",
        "    return np.reshape(state, [1,4])\n",
        "\n",
        "def replay(batch_size, epsilon):\n",
        "    x_batch, y_batch = [], []\n",
        "    minibatch = random.sample(memory, min(len(memory), batch_size))\n",
        "    \n",
        "    for state, action, reward, next_state, done in minibatch:\n",
        "        y_target = model.predict(state)\n",
        "        y_target[0][action] = reward if done else reward + \\\n",
        "            gamma*np.max(model.predict(next_state)[0])\n",
        "            #gives reward if predicted correctly\n",
        "        x_batch.append(state[0])\n",
        "        y_batch.append(y_target[0])\n",
        "    model.fit(np.array(x_batch), np.array(y_batch), \\\n",
        "              batch_size = len(x_batch), verbose = 0) #training the model\n",
        "    \n",
        "    #update the epsilon - progressively get less explorative\n",
        "    if epsilon > epsilon_min: \n",
        "        epsilon *= epsilon_decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef4bJNQZK4u",
        "colab_type": "text"
      },
      "source": [
        "# 5. Defining a run function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SyNg61_Y8Wc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define run function \n",
        "def run():\n",
        "  \"\"\"\n",
        "  records the environment state and use the network to choose the best action\n",
        "  to take\n",
        "  \"\"\"\n",
        "  scores = deque(maxlen =100)\n",
        "  for e in range(n_episodes): #e == episode\n",
        "      state = preprocess_state(env.reset())\n",
        "      done = False\n",
        "      i = 0\n",
        "      while not done:\n",
        "          action = choose_action(state, get_epsilon(e))\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "          #env.render()\n",
        "          next_state = preprocess_state(next_state)\n",
        "          remember(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          i += 1\n",
        "      \n",
        "      scores.append(i)\n",
        "      mean_score = np.mean(scores)\n",
        "      \n",
        "      if mean_score >= n_win_ticks and e >= 100:\n",
        "          if not quiet: print('Ran {} episodes.\\\n",
        "                              Solved after {} trails'.format(e, e-100))\n",
        "          return e - 100\n",
        "      if e % 20 == 0 and not quiet:\n",
        "          print('[Episode {}] - Mean survival time over last 100 episodes was \\\n",
        "          {} ticks'.format(e, mean_score))\n",
        "                \n",
        "      replay(batch_size, get_epsilon(e))\n",
        "      \n",
        "  if not quiet: print('Did not solve after {} episodes'. format(e))\n",
        "  plt.plot(scores)\n",
        "  return e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LU7Y9ARnmvE",
        "colab_type": "code",
        "outputId": "b8a84e9f-bdab-422a-cd1c-16b94ca1171f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "run()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was           15.0 ticks\n",
            "[Episode 20] - Mean survival time over last 100 episodes was           153.23809523809524 ticks\n",
            "[Episode 40] - Mean survival time over last 100 episodes was           123.1951219512195 ticks\n",
            "[Episode 60] - Mean survival time over last 100 episodes was           116.54098360655738 ticks\n",
            "[Episode 80] - Mean survival time over last 100 episodes was           128.7530864197531 ticks\n",
            "[Episode 100] - Mean survival time over last 100 episodes was           123.86 ticks\n",
            "[Episode 120] - Mean survival time over last 100 episodes was           99.14 ticks\n",
            "[Episode 140] - Mean survival time over last 100 episodes was           86.43 ticks\n",
            "[Episode 160] - Mean survival time over last 100 episodes was           76.15 ticks\n",
            "[Episode 180] - Mean survival time over last 100 episodes was           49.68 ticks\n",
            "[Episode 200] - Mean survival time over last 100 episodes was           36.06 ticks\n",
            "[Episode 220] - Mean survival time over last 100 episodes was           43.27 ticks\n",
            "[Episode 240] - Mean survival time over last 100 episodes was           59.34 ticks\n",
            "[Episode 260] - Mean survival time over last 100 episodes was           88.41 ticks\n",
            "[Episode 280] - Mean survival time over last 100 episodes was           120.87 ticks\n",
            "[Episode 300] - Mean survival time over last 100 episodes was           154.56 ticks\n",
            "[Episode 320] - Mean survival time over last 100 episodes was           179.99 ticks\n",
            "Ran 335 episodes.                              Solved after 235 trails\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}